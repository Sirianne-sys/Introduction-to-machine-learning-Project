---
title: "Leaf identification"
author: "MADON KENGNE Sirianne"
date: "2022-12-18"
output: pdf_document
---


## Topic:

The goal is to propose a method for leaf identification based on the provided leaf attributes and using a proper unsupervised or supervised learning tool.

```{r}
#install.packages("tidyverse")
library(tidyverse)
```

```{r}
#install.packages("mlr", dependencies = TRUE)
library(mlr)
```

```{r}
library(ggplot2)
library(cowplot)
library(randomForest)
```


Let's read the data and see the first 6 rows.
```{r}
data <- read.csv("leaf.csv", header=FALSE)
#head(data)
```
Let's give proper columns names to our data
```{r}
colnames(data) <- c("Species","Specimen Number","Eccentricity","AspectRatio","Elongation","Solidity","Stochastic_Convexity","Isoperimetric_Factor","Maximal_Indentation_Depth","Lobedness","Average_Intensity","Average_Contrast","Smoothness","Third_moment","Uniformity","Entropy")

#names(data)
```
Let's give the proper name of species
```{r}
last_species_names<-c(1:15,22:36)
new_species_names<-c("Quercus suber","Salix atrocinera","Populus nigra","Alnus sp.","Quercus robur",
     "Crataegus monogyna","Ilex aquifolium","Nerium oleander","Betula pubescens",
     "Tilia tomentosa","Acer palmatum","Celtis sp.","Corylus avellana","Castanea sativa","Populus alba",16:21,
     "Primula vulgaris","Erodium sp.","Bougainvillea sp.","Arisarum vulgare","Euonymus japonicus","Ilex perado ssp. azorica",
     "Magnolia soulangeana","Buxus sempervirens","Urtica dioica","Podocarpus sp.","Acca sellowiana","Hydrangea sp.","Pseudosasa japonica",
     "Magnolia grandiflora","Geranium sp.")
for(i in last_species_names){
  data[data$Species == i,]$Species <-new_species_names[i] 
} 

#unique(data$Species)
```

let's delete the columns 2 because it is useless for what we want to do
```{r}
dat<-data[,-2]
#head(dat)
```


```{r}
dat$Species <- as.factor(dat$Species)
#str(dat)
```


```{r}
#summary(dat)
```

we will check the number available for each species
```{r}
#table(dat$Species)
```

Let partition our data by creating two independent samples.
```{r}
set.seed(42)
ind<-sample(2, nrow(dat), replace=TRUE, prob=c(0.7,0.3))
train<-dat[ind==1,]
test<-dat[ind==2,]
```


```{r}
#create a learner
set.seed(12345)
forest <- makeLearner("classif.randomForest")
```

```{r}
#changing data frame to task
dataTask <- makeClassifTask(data = train, target = "Species")
```

```{r}
#Tuning the random forest hyperparameters
set.seed(12345)
forestParamSpace <- makeParamSet(
makeIntegerParam("ntree", lower = 200, upper = 500),
makeIntegerParam("mtry", lower = 2, upper = 5),
makeIntegerParam("nodesize", lower = 1, upper = 5),
makeIntegerParam("maxnodes", lower = 5, upper = 20))
```

```{r}
#library(parallel, lib.loc = "/opt/R/4.2.2/lib/R/library")
#library(parallelly)
#library(parallelMap)
```

```{r}
randSearch <- makeTuneControlRandom(maxit = 50)
cvForTuning <- makeResampleDesc("CV", iters = 10)
#parallelStartSocket(cpus = detectCores())
tunedForestPars <- tuneParams(forest, task = dataTask,
resampling = cvForTuning,
par.set = forestParamSpace,
control = randSearch)
#parallelStop()
```

```{r}
tunedForestPars
```

```{r}
#Now let’s train a final model
tunedForest <- setHyperPars(forest, par.vals = tunedForestPars$x)
tunedForestModel <- train(tunedForest, dataTask)
tunedForestModel
```

```{r}
#Plotting the out-of-bag error
forestModelData <- getLearnerModel(tunedForestModel)
species <- colnames(forestModelData$err.rate)
plot(forestModelData, col = 1:length(species), lty = 1:length(species))
#legend("topright", species,
#col = 1:length(species),
#lty = 1:length(species))
```
```{r}
outer <- makeResampleDesc("CV", iters = 5)
forestWrapper <- makeTuneWrapper("classif.randomForest",
resampling = cvForTuning,
par.set = forestParamSpace,
control = randSearch)
#parallelStartSocket(cpus = detectCores())
cvWithTuning <- resample(forestWrapper, dataTask, resampling = outer)
#parallelStop()
cvWithTuning
```
```{r}
cvWithTuning
```

```{r}
xgb <- makeLearner("classif.xgboost")
zooXgb <- mutate_at(train, .vars = vars(-Species), .funs = as.numeric)
xgbTask <- makeClassifTask(data = zooXgb, target = "Species")
xgbParamSpace <- makeParamSet(
makeNumericParam("eta", lower = 0, upper = 1),
makeNumericParam("gamma", lower = 0, upper = 5),
makeIntegerParam("max_depth", lower = 1, upper = 5),
makeNumericParam("min_child_weight", lower = 1, upper = 10),
makeNumericParam("subsample", lower = 0.5, upper = 1),
makeNumericParam("colsample_bytree", lower = 0.5, upper = 1),
makeIntegerParam("nrounds", lower = 20, upper = 20),
makeDiscreteParam("eval_metric", values = c("merror", "mlogloss")))
randSearch <- makeTuneControlRandom(maxit = 10)
cvForTuning <- makeResampleDesc("CV", iters = 5)
tunedXgbPars <- tuneParams(xgb, task = xgbTask,
resampling = cvForTuning,
par.set = xgbParamSpace,
control = randSearch)
tunedXgbPars
```

```{r}
tunedXgbPars
```

```{r}
#Now let’s train a final model
tunedXgb <- setHyperPars(xgb, par.vals = tunedXgbPars$x)
tunedXgbModel <- train(tunedXgb, xgbTask)
tunedXgbModel
```

```{r}
dataPred <- predict(tunedForestModel, newdata = test)
dataPred

```

